{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The real datasets that are used in this notebook can be created by build_datasets.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Set the full path for the PMMSubClusters.exe in windows\n",
    "FULL_PATH_TO_PACKAGE_IN_WINDOWS = os.environ.get('DPMM_GPU_FULL_PATH_TO_PACKAGE_IN_WINDOWS')\n",
    "\n",
    "#Set the full path for the PMMSubClusters in Linux\n",
    "FULL_PATH_TO_PACKAGE_IN_LINUX = os.environ.get('DPMM_GPU_FULL_PATH_TO_PACKAGE_IN_LINUX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system().startswith('Windows'):\n",
    "    if FULL_PATH_TO_PACKAGE_IN_WINDOWS == None:\n",
    "        print('Missing path for windows package. For example: FULL_PATH_TO_PACKAGE_IN_WINDOWS = \"C:/DPMMSubClusters.exe\"')\n",
    "        assert(False)\n",
    "elif platform.system().startswith(\"Linux\"):\n",
    "    if FULL_PATH_TO_PACKAGE_IN_LINUX == None:\n",
    "        print('Missing path for linux package. For example: FULL_PATH_TO_PACKAGE_IN_LINUX = \"/home/user/bin/DPMMSubClusters\"')\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import julia\n",
    "from julia.api import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "# julia.install()\n",
    "from dpmmpython.priors import niw, multinomial\n",
    "from julia import DPMMSubClusters\n",
    "import numpy as np\n",
    "import platform\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior:\n",
    "    def to_julia_prior(self):\n",
    "        pass\n",
    "\n",
    "    def get_type(self):\n",
    "        pass\n",
    "\n",
    "    def to_JSON(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class niw(prior):\n",
    "    def __init__(self, kappa, mu, nu, psi):\n",
    "        if nu < len(mu):\n",
    "            raise Exception('nu should be atleast the Dim')\n",
    "        self.kappa = kappa\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "        self.psi = psi\n",
    "\n",
    "    def to_julia_prior(self):\n",
    "        return DPMMSubClusters.niw_hyperparams(self.kappa, self.mu, self.nu, self.psi)\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'Gaussian'\n",
    "\n",
    "    def to_JSON(self):\n",
    "        j = {'k': self.kappa,\n",
    "             'm': self.mu.tolist(),\n",
    "             'v': self.nu,\n",
    "             'psi': self.psi.tolist()\n",
    "             }\n",
    "\n",
    "        return j\n",
    "\n",
    "\n",
    "class multinomial(prior):\n",
    "    def __init__(self, alpha, dim=1):\n",
    "        if isinstance(alpha, np.ndarray):\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = np.ones(dim) * alpha\n",
    "\n",
    "    def to_julia_prior(self):\n",
    "        return DPMMSubClusters.multinomial_hyper(self.alpha)\n",
    "\n",
    "    def get_type(self):\n",
    "        return 'Multinomial'\n",
    "\n",
    "    def to_JSON(self):\n",
    "        j = {'alpha': self.alpha.tolist()\n",
    "             }\n",
    "\n",
    "        return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPMMPython:\n",
    "    \"\"\"\n",
    "     Wrapper for the DPMMSubCluster Julia package\n",
    "     \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_niw_prior(dim, mean_prior, mean_str, cov_prior, cov_str):\n",
    "        \"\"\"\n",
    "        Creates a gaussian prior, if cov_prior is a scalar, then creates an isotropic prior scaled to that, if its a matrix\n",
    "        uses it as covariance\n",
    "        :param dim: data dimension\n",
    "        :param mean_prior: if a scalar, will create a vector scaled to that, if its a vector then use it as the prior mean\n",
    "        :param mean_str: prior mean psuedo count\n",
    "        :param cov_prior: if a scalar, will create an isotropic covariance scaled to cov_prior, if a matrix will use it as\n",
    "        the covariance.\n",
    "        :param cov_str: prior covariance psuedo counts\n",
    "        :return: DPMMSubClusters.niw_hyperparams prior\n",
    "        \"\"\"\n",
    "        if isinstance(mean_prior, (int, float)):\n",
    "            prior_mean = np.ones(dim) * mean_prior\n",
    "        else:\n",
    "            prior_mean = mean_prior\n",
    "\n",
    "        if isinstance(cov_prior, (int, float)):\n",
    "            prior_covariance = np.eye(dim) * cov_prior\n",
    "        else:\n",
    "            prior_covariance = cov_prior\n",
    "        prior = niw(mean_str, prior_mean, dim + cov_str, prior_covariance)\n",
    "        return prior\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mnmm_prior(alpha, dim):\n",
    "        prior = multinomial(alpha, dim)\n",
    "        return prior\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(data, alpha, prior=None,\n",
    "            iterations=100, verbose=False,\n",
    "            burnout=15, gt=None, outlier_weight=0, outlier_params=None, gpu=True, force_kernel = 2):\n",
    "        \"\"\"\n",
    "        Wrapper for DPMMSubClusters fit, reffer to \"https://bgu-cs-vil.github.io/DPMMSubClusters.jl/stable/usage/\" for specification\n",
    "        Note that directly working with the returned clusters can be problematic software displaying the workspace (such as PyCharm debugger).\n",
    "        :return: labels, clusters, sublabels\n",
    "        \"\"\"\n",
    "        if gpu == True:\n",
    "            np.save(\"modelData.npy\", np.swapaxes(data, 0, 1))\n",
    "\n",
    "            modelParams = {'alpha': alpha,\n",
    "                           'iterations': iterations,\n",
    "                           'use_verbose': verbose,\n",
    "                           'burnout_period': burnout,\n",
    "                           'force_kernel': force_kernel,\n",
    "                           'outlier_mod': outlier_weight,\n",
    "                           'outlier_hyper_params': outlier_params,\n",
    "                           'hyper_params': prior.to_JSON()\n",
    "                           }\n",
    "            if gt is not None:\n",
    "                modelParams['gt'] = gt.tolist()\n",
    "\n",
    "            with open('modelParams.json', 'w') as f:\n",
    "                json.dump(modelParams, f)\n",
    "\n",
    "            if platform.system().startswith('Windows'):\n",
    "                process = subprocess.Popen([FULL_PATH_TO_PACKAGE_IN_WINDOWS,\n",
    "                                            \"--prior_type=\" + prior.get_type(), \"--model_path=modelData.npy\",\n",
    "                                            \"--params_path=modelParams.json\", \"--result_path=result.json\"])\n",
    "            elif platform.system().startswith(\"Linux\"):\n",
    "                process = subprocess.Popen(\n",
    "                    [FULL_PATH_TO_PACKAGE_IN_LINUX,\n",
    "                     \"--prior_type=\" + prior.get_type(), \"--model_path=modelData.npy\", \"--params_path=modelParams.json\",\n",
    "                     \"--result_path=result.json\"])\n",
    "            else:\n",
    "                print(f'Not support {platform.system()} OS')\n",
    "\n",
    "            out, err = process.communicate()\n",
    "            errcode = process.returncode\n",
    "\n",
    "            process.kill()\n",
    "            process.terminate()\n",
    "\n",
    "            with open('result.json') as f:\n",
    "                results_json = json.load(f)\n",
    "\n",
    "            if \"error\" in results_json:\n",
    "                print(f'Error:{results_json[\"error\"]}')\n",
    "                return [], []\n",
    "\n",
    "            os.remove(\"result.json\")\n",
    "            return results_json[\"labels\"], None, [results_json[\"weights\"], results_json[\"iter_count\"]]\n",
    "\n",
    "        else:\n",
    "            if prior == None:\n",
    "                results = DPMMSubClusters.fit(data, alpha, iters=iterations,\n",
    "                                              verbose=verbose, burnout=burnout,\n",
    "                                              gt=gt, outlier_weight=outlier_weight,\n",
    "                                              outlier_params=outlier_params)\n",
    "            else:\n",
    "                results = DPMMSubClusters.fit(data, prior.to_julia_prior(), alpha, iters=iterations,\n",
    "                                              verbose=verbose, burnout=burnout,\n",
    "                                              gt=gt, outlier_weight=outlier_weight,\n",
    "                                              outlier_params=outlier_params)\n",
    "            return results[0],results[1],results[2:]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_ll(points, labels, clusters):\n",
    "        \"\"\"\n",
    "        Wrapper for DPMMSubClusters cluster statistics\n",
    "        :param points: data\n",
    "        :param labels: labels\n",
    "        :param clusters: vector of clusters distributions\n",
    "        :return: vector with each cluster avg ll\n",
    "        \"\"\"\n",
    "        return DPMMSubClusters.cluster_statistics(points, labels, clusters)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_procs(procs_count):\n",
    "        j = julia.Julia()\n",
    "        j.eval('using Distributed')\n",
    "        j.eval('addprocs(' + str(procs_count) + ')')\n",
    "        j.eval('@everywhere using DPMMSubClusters')\n",
    "        j.eval('@everywhere using LinearAlgebra')\n",
    "        j.eval('@everywhere BLAS.set_num_threads(2)')\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_gaussian_data(sample_count, dim, components, var):\n",
    "        '''\n",
    "        Wrapper for DPMMSubClusters cluster statistics\n",
    "        :param sample_count: how much of samples\n",
    "        :param dim: samples dimension\n",
    "        :param components: number of components\n",
    "        :param var: variance between componenets means\n",
    "        :return: (data, gt)\n",
    "        '''\n",
    "        data = DPMMSubClusters.generate_gaussian_data(sample_count, dim, components, var)\n",
    "        gt = data[1]\n",
    "        data = data[0]\n",
    "        return data, gt\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_mnmm_data(sample_count, dim, components, trials):\n",
    "        '''\n",
    "        Wrapper for DPMMSubClusters cluster statistics\n",
    "        :param sample_count: how much of samples\n",
    "        :param dim: samples dimension\n",
    "        :param components: number of components\n",
    "        :param trials: draws from each vector\n",
    "        :return: (data, gt)\n",
    "        '''\n",
    "        data = DPMMSubClusters.generate_mnmm_data(sample_count, dim, components, trials)\n",
    "        gt = data[1]\n",
    "        data = data[0]\n",
    "        return data, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def generate_gaussian_data(n_samples, d, k):\n",
    "    if d > 4:\n",
    "        return DPMMPython.generate_gaussian_data(n_samples, d, k, 0.1)\n",
    "    else:\n",
    "        return DPMMPython.generate_gaussian_data(n_samples, d, k, 100)\n",
    "\n",
    "def generate_mnmm_data(n_samples, d, k):\n",
    "    print(f'start generate_mnmm_data: {datetime.now()}')\n",
    "    return DPMMPython.generate_mnmm_data(n_samples, d, k, 50)\n",
    "\n",
    "def generate_mnist_data(n_samples, d, k):\n",
    "    data = np.load('mnist_images.npy')\n",
    "    pca = PCA(n_components=d)\n",
    "    data = pca.fit(data).transform(data)\n",
    "    data = data - data.mean(axis = 0)\n",
    "    data = data / data.std(axis = 0)\n",
    "    data = np.swapaxes(data, 0, 1)\n",
    "    gt = np.load('mnist_labels.npy').flatten()\n",
    "    return data, gt\n",
    "\n",
    "def generate_fashion_mnist_data(n_samples, d, k):\n",
    "    data = np.load('fashion_mnist_images.npy')\n",
    "    pca = PCA(n_components=d)\n",
    "    data = pca.fit(data).transform(data)\n",
    "    data = data - data.mean(axis = 0)\n",
    "    data = data / data.std(axis = 0)\n",
    "    data = np.swapaxes(data, 0, 1)\n",
    "    gt = np.load('fashion_mnist_labels.npy')\n",
    "    return data, gt\n",
    "\n",
    "def generate_imagenet64_data(n_samples, d, k):\n",
    "    data = np.load('imagenet64_images.npy')\n",
    "    data = np.swapaxes(data, 0, 1)\n",
    "    gt = np.load('imagenet64_labels.npy')\n",
    "    return data, gt\n",
    "\n",
    "def generate_20newsgroups20k_data(n_samples, d, k):\n",
    "    data = np.load('20newsgroups20000_train.npy')\n",
    "    data = np.swapaxes(data, 0, 1)\n",
    "    gt = np.load('20newsgroups20000_labels.npy')\n",
    "    return data, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(n_samples, d, k, numIter = 10, max_iter = 100, model='',\n",
    " get_data=generate_gaussian_data, prior = None, prior_niw_if_none= True, alpha = 1, burnout = 15,\n",
    " force_kernel = 0, run_julia = True, run_cuda = True, run_sklearn = True):\n",
    "    print(f'n_samples={n_samples}, d={d}, k={k}, numIter={numIter}, model={model}: {datetime.now()}')\n",
    "    #Generate sample\n",
    "    data, gt = get_data(n_samples, d, k)\n",
    "    if prior == None:\n",
    "        if prior_niw_if_none:\n",
    "            prior = DPMMPython.create_niw_prior(d, 0, 1, 1, 1)\n",
    "        else:\n",
    "            prior = DPMMPython.create_mnmm_prior(1,d)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df.index.name = 'Iter'\n",
    "\n",
    "    params_str = str(n_samples)+'_'+str(d)+'_'+str(k)\n",
    "    for i in range(numIter):\n",
    "        if run_julia:\n",
    "            #Julia\n",
    "            print(f'Julia...: {datetime.now()}')\n",
    "            labels,_,more = DPMMPython.fit(data, alpha, iterations = max_iter, prior = prior, verbose = False, burnout = burnout, gt = gt, gpu=False)\n",
    "            nmi_result = nmi(gt, labels)\n",
    "            print(f'NMI:{nmi_result}')\n",
    "            df['NMI_'+str(params_str)+'_Julia'+str(i)] = nmi_result\n",
    "            df['Time_elapse_'+params_str+'_Julia'+str(i)] = more[1]\n",
    "\n",
    "        if run_cuda:\n",
    "            #Cuda\n",
    "            print(f'Cuda...: {datetime.now()}')\n",
    "            labels,_,more = DPMMPython.fit(data, alpha, iterations = max_iter, prior = prior, verbose = True, burnout = burnout, gt = gt, gpu=True, force_kernel = force_kernel)\n",
    "            nmi_result = nmi(gt, labels)\n",
    "            print(f'NMI:{nmi_result}')\n",
    "            df['NMI_'+params_str+'_Cuda'+str(i)] = nmi_result\n",
    "            df['Time_elapse_'+params_str+'_Cuda' + str(i)] = more[1]\n",
    "\n",
    "        if run_sklearn:\n",
    "            # Sklearn GM\n",
    "            print(f'Sklearn_GM......: {datetime.now()}')\n",
    "            gm = GaussianMixture(n_components=k, random_state=0, max_iter=max_iter, verbose=0, verbose_interval=1000)\n",
    "            tic = time()\n",
    "            gm.fit(data.T)\n",
    "            gmm_time = time() - tic\n",
    "            labels_pred = gm.predict(data.T)\n",
    "            nmi_result = nmi(gt, labels_pred)\n",
    "            print(f'NMI:{nmi_result}')\n",
    "            df['NMI_'+ params_str+'_Sklearn_GM' + str(i)] = nmi_result\n",
    "            df['Time_elapse_'+params_str+'_Sklearn_GM' + str(i)] = gmm_time\n",
    "\n",
    "            #Sklearn BGM\n",
    "            print(f'Sklearn_BGM......: {datetime.now()}')\n",
    "            if i > 1 and d > 64:\n",
    "                print('Skip on this iteration... too slow')\n",
    "                continue\n",
    "            gm = BayesianGaussianMixture(n_components=k*5, random_state=0, max_iter=max_iter, verbose=0, verbose_interval=1000)\n",
    "            tic = time()\n",
    "            gm.fit(data.T)\n",
    "            gmm_time = time() - tic\n",
    "            labels_pred = gm.predict(data.T)\n",
    "            nmi_result = nmi(gt, labels_pred)\n",
    "            print(f'NMI:{nmi_result}')\n",
    "            df['NMI_'+ params_str+'_Sklearn_BGM' + str(i)] = nmi_result\n",
    "            df['Time_elapse_'+params_str+'_Sklearn_BGM' + str(i)] = gmm_time\n",
    "\n",
    "    path = os.path.join('results','run_result_'+model+'_'+params_str+'.csv')\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run known datasets\n",
    "N = 60000\n",
    "D = 32\n",
    "K = 10\n",
    "repeats = 10\n",
    "max_iter = 100\n",
    "\n",
    "run_test(60000, D, K, repeats, max_iter = 300, model='mnist', get_data=generate_mnist_data, prior=DPMMPython.create_niw_prior(D, 0, 1, 1.46, 456.8))\n",
    "run_test(60000, D, K, repeats, max_iter = 200, model='fashion_mnist', get_data=generate_fashion_mnist_data, prior=DPMMPython.create_niw_prior(D, 0, 1, 1.46, 456.8))\n",
    "run_test(125000, 64, 100, repeats, max_iter = 200, model='imagenet64', get_data=generate_imagenet64_data, prior=DPMMPython.create_niw_prior(64, 0, 1, 0.177459, 720.139))\n",
    "run_test(11314, 20000, 20, repeats, max_iter = 100, model='20newsgroups10k', prior_niw_if_none=False, get_data=generate_20newsgroups20k_data, force_kernel=2, run_sklearn=False)\n",
    "print(f'Complete test: {datetime.now()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate gaussian random data\n",
    "\n",
    "max_iter = 100\n",
    "repeats = 10\n",
    "\n",
    "for N in [1000,10000,100000,1000000]:\n",
    "    for D in [2,4,8,16,32,64,128]:\n",
    "        for K in [4,8,16,32]:\n",
    "            run_test(N, D, K, repeats, max_iter = max_iter, model='generated_gaussian', get_data=generate_gaussian_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate multinomial random data\n",
    "\n",
    "max_iter = 100\n",
    "repeats = 10\n",
    "for N in [1000,10000,100000,1000000]:\n",
    "    for D in [4,8,16,32,64,128]:\n",
    "        for K in [4,8,16,32]:\n",
    "            if D >= K:\n",
    "                run_test(N, D, K, repeats, max_iter = max_iter, model='generate_mnmm', prior_niw_if_none=False, get_data=generate_mnmm_data, run_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Collect all csv files results to one file\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def calculate_nmi_mean(pd,str_to_search):\n",
    "    filter_col = [col for col in pd if str_to_search in col and 'NMI' in col]\n",
    "    sum_value = 0\n",
    "    for i in range(len(filter_col)):\n",
    "        sum_value += pd[filter_col[i]].iloc[-1]\n",
    "    if len(filter_col) > 0:\n",
    "        mean = sum_value/len(filter_col)\n",
    "    else:\n",
    "        mean = -1\n",
    "    return mean\n",
    "\n",
    "def calculate_time_mean(pd,str_to_search):\n",
    "    filter_col = [col for col in pd if str_to_search in col and 'Time_elapse' in col]\n",
    "    sum_value = 0\n",
    "    for i in range(len(filter_col)):\n",
    "        if 'Sklearn' in str_to_search:\n",
    "            sum_value += pd[filter_col[i]].iloc[-1]\n",
    "        else:\n",
    "            sum_value += pd[filter_col[i]].sum(axis=0)\n",
    "    if len(filter_col) > 0:\n",
    "        mean = sum_value/len(filter_col)\n",
    "    else:\n",
    "        mean = -1\n",
    "    return mean\n",
    "\n",
    "def create_all_result_file(result_type, calculate_mean, model):\n",
    "    columns_list = ['Params','Cuda','Julia','Sklearn_GM','Sklearn_BGM', 'X','D','K']\n",
    "    df_all = pd.DataFrame(columns=columns_list)\n",
    "\n",
    "    files = [f for f in listdir('results') if isfile(join('results', f))]\n",
    "\n",
    "    for file in files:\n",
    "        if 'run_result_'+model+'_' in file:\n",
    "            pd_file = pd.read_csv(os.path.join('results',file),index_col=0)\n",
    "            params = file.replace('.csv','').replace('run_result_'+model+'_','')\n",
    "            params_list = params.split('_')\n",
    "            new_row = pd.DataFrame([[params,\n",
    "                                     calculate_mean(pd_file,'Cuda'),\n",
    "                                     calculate_mean(pd_file,'Julia'),\n",
    "                                     calculate_mean(pd_file,'Sklearn_GM'),\n",
    "                                     calculate_mean(pd_file,'Sklearn_BGM'),\n",
    "                                     params_list[0],\n",
    "                                     params_list[1],\n",
    "                                     params_list[2]\n",
    "                                     ]], columns=columns_list)\n",
    "            df_all = df_all.append(new_row, ignore_index=True)\n",
    "    \n",
    "    df_all = df_all.astype({'X': int, 'D': int, 'K': int})\n",
    "    df_all = df_all.sort_values(by=['X','D','K'])\n",
    "    path = os.path.join('results','run_result_all_'+model+'_'+result_type+'_table.csv')\n",
    "    df_all.to_csv(path, index=False)\n",
    "\n",
    "create_all_result_file('NMI', calculate_nmi_mean, 'mnist')\n",
    "create_all_result_file('time', calculate_time_mean, 'mnist')\n",
    "create_all_result_file('NMI', calculate_nmi_mean, 'fashion_mnist')\n",
    "create_all_result_file('time', calculate_time_mean, 'fashion_mnist')\n",
    "create_all_result_file('NMI', calculate_nmi_mean, 'imagenet64')\n",
    "create_all_result_file('time', calculate_time_mean, 'imagenet64')\n",
    "create_all_result_file('NMI', calculate_nmi_mean, 'generated_gaussian')\n",
    "create_all_result_file('time', calculate_time_mean, 'generated_gaussian')\n",
    "create_all_result_file('NMI', calculate_nmi_mean, 'generate_mnmm')\n",
    "create_all_result_file('time', calculate_time_mean, 'generate_mnmm')\n",
    "create_all_result_file('NMI', calculate_nmi_mean, '20newsgroups10k')\n",
    "create_all_result_file('time', calculate_time_mean, '20newsgroups10k')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "interpreter": {
   "hash": "34b414f817d4cb14ae50d2374de9fe6611a9d65cd3430da6f5e12d03a6418fd7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}